---
layout: single
title: 경사 하강법
tags: [gradient_descent]
categories: ml_basic
---

# Introduction
ML/DL에서 비용함수를 최소화시키기 위해 가장 기본이 되는 optimizer인 경사하강법(GD)과 
이것에 파생된 방법들을 설명한다.

# Pre-question
- 경사하강법은 무엇인가?
- 다변수 함수에 대해 경사하강법을 적용하는 방법을 설명하라.
- 선형회귀(linear regression)에서 목적식을 최소화하는 방법을 설명하라.
- 경사하강법을 사용해도 극소값에 도달하지 못하는 경우가 있는데 어떤 경우일까?
- 극소값에 도달하기 위해 개선된 방법이 어떤 것이 있는지 설명하라.

# 경사하강법
경사하강법은 목적함수의 최소값을 구하는 방법 중 하나로, **주어진 x에 대해 기울기를 빼가면서** 최소값을 구한다.
단변수 함수의 경우 다음과 같은 절차를 가진다.

1. 점의 x 좌표가 주어진다. (초기조건)
어느 방향으로 가야 목적함수가 최소가 되어야 할지 결정해야 한다.
2. 목적 함수 f에 대해 주어진 점 (x, f(x)) 에서의 접선의 기울기를 구한다.
한 점에서 접선의 기울기를 알아야 어느 방향으로 점을 움직여서 함수값이 증가/감소할지 알 수 있다.
3. 주어진 점 x에 기울기를 뺀다. **기울기를 뺀다는 것은 함수가 증가하는 방향의 반대방향으로 간다**는 것이고, 이는 극소값으로 향함을 의미한다. 
4. 기울기가 일정 수준 이하가 되기 전까지 2~3을 반복한다.

아래와 같이 의사코드를 짤 수 있다. 
init은 x좌표의 초기값, gradient는 x좌표에서 목적함수의 기울기이다.
eps는 경사하강법을 멈추기 위한 최소 기울기 조건이다.
lr은 learning rate로 x 좌표의 step 사이즈를 결정한다.
```python
var = init
grad = gradient(var)

while(abs(grad) > eps):
  var = var - lr * grad
  grad = gradient(var)
```

# 다변수 함수에서 경사하강법
## Norm
Norm은 벡터의 크기를 일반화한 표현이다.
∥x∥ 라고 표현하며, 기하학적으로는 원점과 점 x와의 거리이다.
L1, L2, Lx 노름이 있는데, L2이 우리가 보통 생각하는 거리를 계산할 때 쓰이며 유클리디안 거리라고 한다.   
경사하강법에서는 노름으로 벡터의 크기를 표현하기 위해 사용한다.

![](./../../../assets/images/(TODO)2022-09-19-Log_images/1663641887464.png)

## 그레디언트
벡터가 입력인 다변수 함수의 경우 **기울기를 구하기 위해서 편미분과 그레디언트(gradient)를 사용한다.**
각 변수 별로 편미분을 계산한 그레디언트 벡터를 이용해 점 x의 좌표를 업데이트할 수 있다.
수식에서 ∇는 navia 기호라고 부른다.

![](./../../../assets/images/(TODO)2022-09-19-Log_images/1663642618240.png)

다변수 함수의 경우에도 기존의 알고리즘이 그대로 적용되나, 그레디언트 벡터의 크기인 노름을 계산해서 종료 조건을 설정한다.

```python
var = init
grad = gradient(var)

while(norm(grad) > eps):
  var = var - lr * grad
  grad = gradient(var)
```
 
# 다변수 함수에서 경사하강법


# Discussion
## 요약


## 중요하다고 생각하는 부분


## 활용할 수 있는 부분


## 관련 개념


## 기타 


# Reference

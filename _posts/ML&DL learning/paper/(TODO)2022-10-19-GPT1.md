---
layout: single
title: Improving Language Understanding by Generative Pre-Training 논문 리뷰
tags: [paper-review, GPT]
categories: paper
---

# Paper Title

Improving Language Understanding by Generative Pre-Training


## Keyword

behavior recognition, Livestock, EffecientNet, BiFPN, LSTM

## Summary   

라벨링된 말뭉치(Corpora)가 매우 풍부하지만, 이 때문에 모델을 적절히 훈련시키는 것이 쉽지 않다.


# Introduction
NLP 도메인에서 학습을 위해 데이터에 모두 labeling을 하는 것은 상당한 cost를 요구한다.
따라서 unlabeled dataset에서 정보를 추출할 수 있는 모델이 상당히 가치있어졌다.

## 이슈
그렇지만  데이터로 학습을 하는데에는 두 가지 어려움이 있다.

1. 모델이 유용한 text representation을 익히는데 필요한 목적함수를 정의하기가 어려움.
2. 학습된 representation을 target task에 전이학습하기 위해 합의된 방법이 아직 없다.

unsupervised pre-trainig과 supervised fine-tuning을 결합한 semi-supervised learning으로
모델링에 접근하고자 한다

two-stage로 trainig 과정을 접근한다.
1. 모델의 초기파라미터를 학습하기 위해 unlabeled data에 대한 학습을 진행한다.
2. target task를 위해 label을 이미 아는 데이터로 지도학습을 진행한다.

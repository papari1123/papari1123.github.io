---
layout: single
title: RNN, LSTM, GRU
tags: [RNN, LSTM, GRU]
categories: ml_basic
---
# Introduction
이미지는 

# Pre-question
- RNN보다 LSTM에서 gradient vanishing/exploding 현상이 덜 나타나는 이유는?
- LSTM의 gate에 시그모이드를 쓴 이유, hidden state에서는 tanh를 쓴 이유?
- RNN 계열에서 ReLU를 안쓰는 이유는?
- 같은 task에서 GRU가 LSTM보다 좋은 결과를 내는 이유를 뭘로 추측할 수 있을까?     
- LSTM과 GRU의 파라미터 개수는 어떻게 결정될까?

주어진 입력이 시퀀셜 모델이다.
이게 뭔가..

시퀀셜 데이터를 다루는 데 어려움.

제일 큰 어려움. 얻고 싶은 거는
하나의 라벨인데, 내가 원하는 건 --이다.

근데 입력 데이터의 길이가 언제 끝날지 몰라. 차원을 알 수가 없어.

그러면 시퀀셜 모델에 대해 생각을 해보자.
가장 간단한 건 랭귀지 모델..
즉 입력데이터가 얼마나 클지 몰라..
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664938253535.png)

그래서 제일 쉽게 하는 건 과거의 N개만 보는 것,
훨씬 계산이 쉬워진다.
이게 마르코프 모델
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664938395796.png)
나의 현재는 과거에만 의존한다.
하지만 말이 안돼.. 과거에 누적된 정보들을 모두 고려해야 하기 때문.

아래 모델은 히든 스테이트가 있어 과거 정보를 담고 있다.
히든 스테이트가 과거 정보를 담는다.. -> RNN
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664938471307.png)

![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664938618186.png)

숏텀 디펜던시: 내가 과거에 얻은 정보가 다 취합되서
미래를 고려해야 하는데, rnn은 계속 취합해서 그게 살아남기 힘들다.

길게 고려하자..

W를 T번 곱하게 돼.. 정보가 안 넘어가..
이래서 ReLu를 안 쓰기도 한다.
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664938724144.png)


![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664943534482.png)

![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1665626442348.png)
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1665626451431.png)
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1665626603633.png)
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1665626692793.png)
ht는 다음 RNN의 hidden state로 넘어간다. 그리고 C는 -1~1 ->  현재 state cell의 정보를 필터링한다.
state cell의 정보를 지우지는 않는 것.


cell state candidate 

cel state vector : 필요한 정보를 포함한다.
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1665624834205.png)


![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664943979201.png)
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664943985092.png)

GRU
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664943889183.png)

cell state와 hidden state vector를 일원화했음.


cell state 
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1664944055643.png)

덧셈으로 인해 사라진다.
-> backpropagation에서 gradient를 복사해주는 역할을 한다.
![](./../../../assets/images/(TODO)2022-10-05-RNN2_images/1665627203911.png)


똑같은 테스크에 대해 GRU를 사용할 때 LSTM
적은 파라미터를 써도,
# Discussion
- ResNet residual, LSTM cell state 전달 차이
https://powerofsummary.tistory.com/133
- 
- LSTM의 gate에 시그모이드를 쓴 이유, hidden state에서는 tanh를 쓴 이유?    
: gate는 값의 정도를 조절하기 위해 시그모이드, hidden state 계산 시 tanh를 쓴 이유는 gradient vanishing 현상 줄이기 위해 
- RNN보다 LSTM에서 gradient vanishing/exploding 현상이 덜 나타나는 이유는?
- 
- RNN 계열에서 ReLU를 안쓰는 이유는?
: 일단 relu의 경우 값이 0부터 무한대라, hidden state값이 발산할 수 있다.
- 같은 task에서 GRU가 LSTM보다 좋은 결과를 내는 이유를 뭘로 추측할 수 있을까?
: 파라미터 수를 줄어서 일반화 능력 이 늘어남?

# Reference
- 네이버 AI 부트캠프 (* 강의 자료 바탕으로 재구성)            
- [LSTM activation](https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm)


model.eval

eval해서 batchnorm, dropout

torch.no_grad
* 
* 포인트


- **RNN보다 LSTM에서 gradient vanishing/exploding 현상이 덜 나타나는 이유는?**
    
    
    ****아래는 출처에 나온 내용 중 일부 식이며, 시그모이드를 미분한 항이 time_length k만큼 계속 곱해지는 것을 볼 수 있음
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dfb2aa70-10ce-45f7-9543-eecf8029ceba/Untitled.png)
    
    출처: [https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)
    
    LSTM 식 참고 (아래)
    

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c244cfd3-5572-4e8d-8972-000cc8d811ad/Untitled.png)

- **LSTM의 gate에 시그모이드를 쓴 이유, hidden state에서는 tanh를 쓴 이유?**
 위 식에서 LSTM gate 는 input, forget, output gate는 각각 입력, 이전상태, 출력값(RNN 출력식과 동일)을 얼마나 최종 결과 st(cell state까지 고려한 값)에 반영할지를 나타내는 값으로 0~1의 값을 가지는 것이 타당함. 따라서 0~1 범위 내의 출력값을 가지는 시그모이드를 사용함.
 반면 최종 출력 st에 대해서 vanishing gradient 현상을 줄이기 위해, tanh를 사용할 필요가 있음.
tanh의 미분값은 모든 정의역에서 sigmoid보다 더 큰 값을 가지기 때문에, gradient가 좀 더 천천히 0으로 수렴하게 됨.

참고: 
****[https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm](https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm)

- **RNN 계열에서 ReLU를 안쓰는 이유는?**
”*At first sight, ReLUs seem inappropriate for RNNs because they can have very large outputs so they might be expected to be far more likely to explode than units that have bounded values.*”
참고 : [https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)

**ReLU는 0~무한대의 값을 가짐**.  아래 이미지처럼 recurrent unit에서 time_length 만큼 계속 같은 WT가 곱해지는데 Relu의 입력값이 1이상일 경우, **hidden state 값이 무한대로 발산할 위험이** 0~1 (시그모이드)또는 -1~1(탄젠트)로 제한되는 **다른 activation 함수보다  큼.**



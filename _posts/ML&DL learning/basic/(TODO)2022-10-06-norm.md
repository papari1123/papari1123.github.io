---
layout: single
title: norm 레이어
tags: [deep learning, batch norm, layer norm, norm]
categories: ml_basic
---
# Introduction
딥러닝에서 normalization은 **보통 데이터 간의 스케일 차이를 제거하기 위해서 쓰인다.**    
그러나 데이터에 따라 normalization을 하는 방법이 다양하며, normalization을 하는 의미도 약간 달라질 수 있다.          
이번 포스팅에서는 주로 사용되는 normalization 방법들을 알아보려고 한다.

# Pre-question
- Batch norm의 장점과 단점은?
- Transformer에서는 BN 대신 왜 Layer norm을 쓸까?

# Normalization 종류
아래는 Group Norm 논문에서 가져온 그림이다. 파란색으로 색칠한 영역이 같은 normalization 대상으로 묶이며, 동일한 평균과 분산으로 normalization 된다.
![](./../../../assets/images/(TODO)2022-10-06-norm_images/1665031623394.png)
- H와 W는 spatial 차원을 나타내며, H와 W를 쌍으로 묶어 특별히 구분하지 않았다. 3차원 이상의 공간에서 데이터가 정의된다 하더라도 동일하게 생각하면 될 듯하다.
- C은 데이터의 channel, 이미지를 예시로 들면 R,G,B이고 NLP는 임베딩 차원이 되겠다.
- N은 미니배치 사이즈를 나타내며, 이 축은 미니 배치에 포함된 샘플 1~N를 나타낸다.       

위 그림을 이해하기 어려울 수 있어, Batch-norm과 Layer-norm을 비교한 다른 그림을 예시로 가져왔다.


![](./../../../assets/images/(TODO)2022-10-06-norm_images/1665417513926.png)

## Batch-Norm
미니 배치의 모든 샘플에 대해 각 채널 별로 normalization을 진행한다.    
loss function의 smoothness를 증가시켜 안정적인 최적화를 위해 사용된다고 한다.    
초기 CV 도메인에서는 Batch norm을 많이 사용하였다. **그러나 batch 사이즈가 줄어들수록 성능이 안좋아진다고 한다.**    


## Layer-Norm
1개 샘플 단위로 모든 channel를 포함해 normalization을 진행한다.      
요즘 많이 쓰이는 Transformer에서는 Layer Norm을 사용한다. 저자가 왜 LN 쓰는지를 논문에 쓰여있지 않았기 때문에, LN을 사용하는 이유를 간단히 추측하자면
 Transformer에 레이어가 매우 많은데, 출력값 분포가 레이어를 거칠 때마다 점점 달라질 것이기 때문에

## Instance-Norm


## Group-Norm



# Reference
- 네이버 AI 부트캠프 (* 강의 자료 바탕으로 재구성)
- [배치 정규화에 대하 정리 블로그](https://eehoeskrap.tistory.com/430)
- [여러 norm에 대한 정리 블로그](https://dongsarchive.tistory.com/74)
---
layout: single
title: RNN, LSTM, GRU
tags: [RNN, LSTM, GRU]
categories: ml_basic
---
# Introduction
이미지는 


# Pre-question


INPUT에 대한 distribution을 학습한다.
샘플링된 x를 분포내 에 있다

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665115626586.png)

Generation
Density estimation : explicit



![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665115648786.png)

설명 예시; 하나의 픽셀의 값을 나타낼 때 가능한 경우의 수는 256x256x256
case = 사건이라고 할 때 이 픽셀값을 확률 분포(?)로 완벽히 나타내기 위해 필요한파라미터 수는 256x256x256-1
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665117480163.png)
-> 매우 많다.
이걸 줄이기 위해 필요한 성질은 "independent"

픽셀값이 0, 1만 가진다고 하자. X1,... Xn이 다 독립이면
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665117743084.png)
왜 파라미터 수가 2^ㅜ - 1 이 아니라 n이지?
사건이 서로 독립이라

근데 이런 경우가 많지는 안ㅇ하서 유의미 하진 않아..

그래서 나온 거
조건적 독립

체인룰, 베이즈룰 : 항상 만족함 

조건적 독립ㅈ 
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665117189723.png)
조건적 독립
: xㅗy | z : z가 주어졌을 때만 x,y가 독립적
ㅗ : 펄프라고 읽는다.
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665117332721.png)



![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665118150013.png)

아래, Xi+1 은 Xi이 주어질 때 Xi-1~X1과 독립이다. 
P(xn|Xn-1) = xn-1 = 1일때, xn-1= 0일 때 각각 한개의 파라미터
= 2n - 1
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665118191852.png)

즉 컨디셔녈 independent 구조를 주면서

조건적 독립으로 아래 joint distribution을 쪼갠다.
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665118482856.png)

autoregressive 모델은 1차원으로 쪼갬


explicit 모델 : 주어진 이미지가 얼마나 모델링하는 것에 density랑 비슷한ㄱ.

장점
- 첫번 째 핸들링 -> 다음 꺼 시퀀셜 하게 샘플링한다. 즉, 샘플링이 쉽다.
- explicit하다. 주어진 데이터에 대해 확률 분포값, (joint distributuon)를 구할 수 있다.
- 
단점
784번의 시퀀셜 오퍼레이션을 거쳐야 한다. 병렬화 안된다. 생성 느리다.

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665118673537.png)

MLE
강아지를 생성해 내는 분포가, 모르지만
존재한다고 가정하자.

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665119396132.png)

라면이 맛있다 라는 것을 정의

라면이 맛있다 = 맛있다는 걸 어떻게 정의할 거야

이런게 있다..
수식이 있다. 근사적으로 두 확률 분포 사이의 거리.
Pdata는 뭔지는 모르지만 강아지를 설명하는 확률 분포다.
P세타 = 

generative 모델을 잘 만드는 건 -> likelihood를 높이는 것으로 귀결된다.
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665119580161.png)

모든 실제 data에 대해 할 수 없으니, D라는 데이터셋(모은 데이터)를 이용해서
우도가 높아지도록 한다.
분산에 대한 설명 : T(샘플 개수)가 적다면 variance는 높아질 수 있따.
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665119959990.png)

한정된 데이터로 오버피팅 등이 일어날 수 있어 경험적 위험 감소 ERM가 필요.
극단적으로는 강아지 데이터 100개가 주어지면 100개를 다 외워서 그 100개 만 생성하는 것이다.

그래서 모델 스패이스(hypothesis space)를 줄이게 된다.
ERM
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665120072056.png)

이미지와 같은 고차원 공간을 시도 했지만 잘 안되었어.

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665120371657.png)


![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665120444237.png)


VAE  Variational Autoencoder
Is an autoencoder a generative model?
: 오토엔코더는 모델ㅇ지만 GENERATIVE 모델은 아니다

목적은 간단하다 : p세타(x)를 최대화


내가 찾을 수 있는 분포로 근사화

posterior 분포 : 데이터가 주어졌을 때 파라미터의 확률 분포
variational 분포 : 상대적으로 간단한 분포 > posterior 분포에 근사
근데 잘 모르는데 어떻게 근사항냐
![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665120624142.png)

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665120909992.png)

아래 q(posterior, 뭔지 모를 분포)
Variation Gap을 줄이기 위해서,
ELBO를 최대화 한다.

사실 log p 세타를 키우고 싶은데..

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665120948524.png)

p(z)는 prior

Recpnsturciton

![](./../../../assets/images/(TODO)2022-10-07-GAN_images/1665121333747.png)

Prior Fitting Term
-> 줄여어..

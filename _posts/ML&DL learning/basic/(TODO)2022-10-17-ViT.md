---
layout: single
title: Transformer
tags: [Transformer]
categories: ml_basic
---
# Introduction
이미지는 

saturating performance 의 경향도 나타지 않음.
: 네트워크 규모가 커진다고 해도 성능이 더 높아지지 않는 것.

VIT 장단점.

![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130030157.png)


![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130161730.png)

inductive bias
![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130211599.png)



![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130359345.png)
![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130459053.png)

1. 이미지를 patch로 단위화
2. linear projection of flattened patches
3. class token 추가, positional embedding
4. Transformer encoder 통과
5. MLP  Head 통과
6. Class 분류
![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130667638.png)

![](./../../../assets/images/(TODO)2022-10-05-Transformer_images/1665130836730.png)

____

코드 연산 확인하는 거
https://nn.labml.ai/transformers/index.html


# Pre-question

# Reference
- 네이버 AI 부트캠프 (* 강의 자료 바탕으로 재구성)   




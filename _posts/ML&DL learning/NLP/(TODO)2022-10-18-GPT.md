---
layout: single
title: NLP 기초
tags: [NLP]
categories: nlp
---
# Introduction
Self-Supervised Pre-Training Models
- GPT-1
- BERT
- 
# Pre-question

self-attention block는 decoder, encoder에 기본적으로
쓰임.

transfer learning, fine tuning을 통해 NLP 분야에서 예측 성능이 크게 상승했다. 

# GPT-1
- 다양한 스폐셜 토큰으로, 다양한 TASK를 동시에 커버할 수 있는 통합된 모델.
  (아래 start, extract, delim이 스폐셜 토큰이다.)
- 문장 레벨, 다수의 문장이 존재하는 경우도 모델이 큰 변형없이 사용할 수 있도록 프레임워크를 설정.

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666083681253.png)
extract 토큰이 쿼리로 사용되서 -. task에 필요로 하는 여러 정보를 적절하게 추출할 줄 알아야 한다.

Classification까지는 초기학습을 진행하는 거
그 뒤 entailment, multiple choice들은 transfer learning으로 lr을 줄여서 학습한다.
그렇게 해서 큰 변화가 일어나지 않도록 한다.
이전 단계에서 배웠던 여러 지식을 활용할 수 있도록 한다.

그 결과, finetuning 한 결과가 좀 더 좋게 나왔다.
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666084701373.png)

# BERT 모델
Bi-LSTM
: motivate는 뭔가?
기존 GPT의 경우
<SOS> I study math.
-> 전후 문맥을 보질 않고 앞 문장만 본다.

그래서 나온 것이 Masked Language Model
: 각 단어들을 mask로 치환하고 그 단어를 맞추는 것.
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666084801352.png)

마스크를 치환하는 비율이 클 경우, 마스크 단어를 맞추기 위한 충분한 정보가 제공되지 않을 수 있다.
비율이 적을 경우, 예를 들어 100개 중 1개 단어만 맞춘다면, 전체 정보를 인코딩 하는 과정에 대한
loss를 고려할 때 1개만 맞추는 건 효율적이지 않음.

마스크로 치환한다.

여러 다른 학습 방법들이 있다.
아래 처럼 15% 워드를 또 나눠서 mask로 쓰거나, random word, same sentence
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666085252017.png)

SEP: 문장이 끝났다는 듯
CLS: GPT에서 extract 토큰이 다수 문장의 마지막에 등장하는데, 다수 문장에 대한 예측 결과를 낼 수 있도록 하는 것인데,
bert에서는 이걸 cls로 씀
cls는 해당하는 인코딩 벡터를 가지고, 역시 output layer 하나를 둬서 binary classification을 할 수 있도록 한다.

버트 모델 :

L- self-attention block 개수, H-  모델 인코딩 벡터의 차원, A - MHA HEAD 개수
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666085751483.png)

두개의 문장은 SEP으로 구분했는데, 
포지션 임베딩이 추가적으로 더해질텐데.
또다른 임베딩으로 문장레벨에서의 포지션을 추가한다.
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086023199.png)

각 timestep에서는 
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086184801.png)

# BERT와 GPT 차이

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086405467.png)

기계 독해에 기반한 QA시스템

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086881968.png)


gradient descent 
다수의 많은 양의 데이터로 

# SQuAD
스텐포드 퀘스쳔 answering dataset

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666087994760.png)
이런 질문과 답변을 concat해서 사용

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666088293211.png)

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666088457119.png)

# Attention


# Discussion


# Reference
- 네이버 AI 부트 (* 강의 자료 바탕으로 재구성)

---
layout: single
title: NLP 기초
tags: [NLP]
categories: nlp
---
# Introduction
Self-Supervised Pre-Training Models
- GPT-1
- BERT
- 
# Pre-question

self-attention block는 decoder, encoder에 기본적으로
쓰임.

transfer learning, fine tuning을 통해 NLP 분야에서 예측 성능이 크게 상승했다. 

# GPT-1
- 다양한 스폐셜 토큰으로, 다양한 TASK를 동시에 커버할 수 있는 통합된 모델.
  (아래 start, extract, delim이 스폐셜 토큰이다.)
- 문장 레벨, 다수의 문장이 존재하는 경우도 모델이 큰 변형없이 사용할 수 있도록 프레임워크를 설정.

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666083681253.png)
extract 토큰이 쿼리로 사용되서 -. task에 필요로 하는 여러 정보를 적절하게 추출할 줄 알아야 한다.

Classification까지는 초기학습을 진행하는 거
그 뒤 entailment, multiple choice들은 transfer learning으로 lr을 줄여서 학습한다.
그렇게 해서 큰 변화가 일어나지 않도록 한다.
이전 단계에서 배웠던 여러 지식을 활용할 수 있도록 한다.

그 결과, finetuning 한 결과가 좀 더 좋게 나왔다.
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666084701373.png)

# BERT 모델
Bi-LSTM
: motivate는 뭔가?
기존 GPT의 경우
<SOS> I study math.
-> 전후 문맥을 보질 않고 앞 문장만 본다.

그래서 나온 것이 Masked Language Model
: 각 단어들을 mask로 치환하고 그 단어를 맞추는 것.
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666084801352.png)

마스크를 치환하는 비율이 클 경우, 마스크 단어를 맞추기 위한 충분한 정보가 제공되지 않을 수 있다.
비율이 적을 경우, 예를 들어 100개 중 1개 단어만 맞춘다면, 전체 정보를 인코딩 하는 과정에 대한
loss를 고려할 때 1개만 맞추는 건 효율적이지 않음.

마스크로 치환한다.

여러 다른 학습 방법들이 있다.
아래 처럼 15% 워드를 또 나눠서 mask로 쓰거나, random word, same sentence
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666085252017.png)

SEP: 문장이 끝났다는 듯
CLS: GPT에서 extract 토큰이 다수 문장의 마지막에 등장하는데, 다수 문장에 대한 예측 결과를 낼 수 있도록 하는 것인데,
bert에서는 이걸 cls로 씀
cls는 해당하는 인코딩 벡터를 가지고, 역시 output layer 하나를 둬서 binary classification을 할 수 있도록 한다.

버트 모델 :

L- self-attention block 개수, H-  모델 인코딩 벡터의 차원, A - MHA HEAD 개수
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666085751483.png)

두개의 문장은 SEP으로 구분했는데, 
포지션 임베딩이 추가적으로 더해질텐데.
또다른 임베딩으로 문장레벨에서의 포지션을 추가한다.
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086023199.png)

각 timestep에서는 
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086184801.png)

# BERT와 GPT 차이

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086405467.png)

기계 독해에 기반한 QA시스템

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666086881968.png)


gradient descent 
다수의 많은 양의 데이터로 

# SQuAD
스텐포드 퀘스쳔 answering dataset

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666087994760.png)
이런 질문과 답변을 concat해서 사용

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666088293211.png)

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666088457119.png)

# Attention


# Discussion


# Reference
- 네이버 AI 부트 (* 강의 자료 바탕으로 재구성)

____멘토링___

Generative : 단어 하나하나를 생성해내는 것을 
Distcriminate :classifier다.

GPT는 왜 Generative?    
optimizer가 다르다. => finetuning과 pretrain이 다르다.

SGD : 

optimizer 에서 특정 파라미터만 사용한다.
원하는 부분만 학습하기 위해

텍스트를 만들어줘야 하기 때문에

히든 스테이트에 텍스트 인코딩을 곱해져서 -> 텍스트를 만들어주는 거다.

학습할때 vocab 개수가 동일하고, 한번 T 시점에서 vocab
-> a와 b를 곱하면 a와 b의 유사도를 구하는 것임.

softmax(hW) -> hw곱해서 -> W는 weight라기 보단, 유사도를 구하기 위해 쓰는 거임.



bert는 인코딩 모델 -> 이걸로 유사도를 계산하기 좋다. 생성몸델은 유사도 계산하가ㅣ 어렵다


GELU

- 알고있는 Relu 보다 안정적
-> Relu는 꺾여있는 부분이 있다.
ELU는 각진 것들을 모두 없애자.

pre-training 하고 zero-shot 하는 이유:
특정 task에 얽매이지 않고, 언어모델이 language를 얼마나 이해하고 있는지를 알 수 있다.
lower bound : 실험 설계 시 중요하다.

knowledge probing

task에 따라 다르다.



__
맥북에서 쓰고 있는 컴퓨터
화면을 띄우면 
먄 앞이
Remote -SSH : 원격 접속
주피터 랩

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666324913594.png)

dataet

아마존 리뷰
바이오매디컬
페이퍼 데이터

git graph/
git history/
git lens/
indent-rainbow
/jupyter/
pylance/
raindow brack

formatting

formatting
black formatter

코드 구현 시 팁

학습할 때 잘 돌아가는지 안돌아가는지
-> 1 epchㄹ를 돌리는 걸 목표로.
일단 셔플하지 않고
모든 과정의 메인 프로세스에는 try except 하여
인덱스가 나오도록.

1.
하나의 프로그램을 만든다고 생ㄱ가하고,

다 py 파일로 엮어서 러닝할 수 있도록 한다.

- shape 맞추기..
- 다 프린트해서 체크를 한다.

노트북에서 테스트 코드를 다 짜놓고 함.

다 모듈화 해놓는 연습을 많이 해두자.

2. config
: argparse를 쓴다.


args = parser.parse_args([])
pyyaml

template 파일 만든다.
박정아 멘토님.

log파일을 찍어서 확인.

노션에 

실험결과 페이지를 노션으로 만든다.

노션을 정리해보자..
![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666326244793.png)


엑셀파일

노션


깃
: 레포지토리 - 템플릿을 만들면 어떨까



모듈별로 폴더를 만들고 누가 담당자인지
디렉토리 설명 다 적고
어떻게 해야하는지 결과를 저장

![](./../../../assets/images/(TODO)2022-10-18-GPT_images/1666326441052.png)
src, config, scripts
captuion_result


Readme Pattern 이 있다.

노트북 폴더를 만들어서 같이 찍어본다.


MODULE
├── result : 모듈별 결과물이 저장되는 장소로 output과 학습된 모델이 저장됩니다
├── script : 모듈 실행을 위한 script 파일
├── src : 모듈 구성을 위한 소스코드
└── README.md : 각 모듈을 실행하기 위한 자세한 가이드라인
DATASET: 분석에 사용할 데이터셋


지난 기수들이 어떻게 코드를 구성했느지

프로젝트 할 때는 각자 브랜치를 타서 할 것..

그 기능 

** Discussion 기능 : 친구랑 일할 때 중요하다..

https://github.com/bcaitech1/p3-mrc-vumblebot

https://github.com/bcaitech1/p3-dst-chatting-day/discussions

PR을 위한 모든 과정
https://github.com/ohsuz 
: 어떻게 프로젝트를 관리하는지

https://github.com/bcaitech1/p4-dkt-freshtomato  
: Discussion

이전 선행 지수에 차이


다ㅣ음주 에는 멘토링 1시간 반 


주중에는 
코어 구
fast api
streamlit

변성윤
mlops

gpu늖 ㅏㄴ정되어 있는..
실제로 서비스되고 있는 건..

추론 속도를 개선할 때, 
***
뭐가 덜 중요한지 결정하자..


bert / t5
구글 bert
페이스북 t5
BART-large

한국어-> klue roberta large (더 좋다)

koelectra

거기에 코드가 다 있는데,
attention 모듈 코드가 있다.

모듈화

bert 계열로 사용..
